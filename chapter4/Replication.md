# 4.1 카프카 리플리케이션

고가용성 분산 스트리밍 플랫폼인 카프카는 무수히 많은 데이터 파이프라인의 정중앙에 위치하는 메인 허브 역할을 합니다.  
이렇게 중앙에서 메인 허브 역할을 하는 카프카 클러스터가 만약 하드웨어의 문제나 점검 등으로 인해 정상적으로 동작하지 못한다거나, 카프카와 연결된 전체 파이프라인에 영향을 미친다면 이는 매우 심각한 문제가 아닐 수 없습니다.  

따라서 카프카는 초기 설계 단계에서부터 이러한 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적인 서비스가 운영될 수 있도록 구상됐습니다.  
이 때 안정성을 확보하기 위해 카프카 내부에서는 리플리케이션이라는 동작을 하게 됩니다.  

## 4.1.1 리플리케이션 동작 개요  

카프카는 브로커의 장애에도 불구하고 연속적으로 안정적인 서비스를 제공함으로써 데이터 유실을 방지하며 유연성을 제공합니다.  
카프카의 리플리케이션 동작을 위해 토픽 생성시 필숫값으로 replication factor 라는 옵션을 설정해야 합니다.  

카프카는 리플리케이션 팩터라는 옵션을 이용해 관리자가 지정한 수만큼의 리플리케이션을 가질 수 있기 때문에 N개의 리플리케이션이 있는 경우 N - 1까지의 브로커 장애가 발생해도 메시지 손실 없이 안정적으로 메시지를 주고받을 수 있습니다.  

총 3개의 리플리케이션이 있다면 그 중 2대의 브로커 장애가 발생하더라도 남은 1대의 브로커가 클라이언트들의 요청을 안전하게 처리할 수 있게 됩니다.  

## 4.1.2 리더와 팔로워  

토픽 상세보기 명령어를 실행해보면 출력 내용 중 파티션의 리더라는 부분이 있습니다.  
모두 동일한 리플리케이션이라 하더라도 리더만의 역할이 따로 있기 때문에 카프카에서 리더를 특별히 강조해 표시합니다.  

카프카는 내부족으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분하고, 각자의 역할을 분담시킵니다.  
리더는 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 그 리더를 통해서만 가능합니다.  
다시 말해, 프로듀서는 모든 리플리케이션에 메시지를 보내는 것이 아니라 리더에게만 메시지를 전송합니다.  
또한 컨슈머도 오직 리더로부터 메시지를 가져옵니다.  

peter-test01 토픽의 파티션 수는 1이고, 리플리케이션 팩터 수는 3일 때, 파티션 번호도 함께 표시하는데, 예를들어 peter-test01-0은 peter-test01 토픽의 0번 파티션이라는 의미입니다.  

프로듀서는 peter-test01 토픽으로 메시지를 전송하는데, 파티션의 리더만 읽고 쓰기가 가능하므로 0번 파티션의 리더로 메시지를 보냅니다.  
컨슈머 동작에서도 역시 0번 파티션의 리더로부터 메시지를 가져옵니다.  

그렇다면 파티션은 어떤 동작을 수행할까요?  
팔로워들은 그저 대기만 하는 것이 아니라, 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 해야 합니다.  
따라서 컨슈머가 토픽의 메시지를 꺼내 가는 것과 비슷한 동작으로 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제합니다.  

## 4.1.3 복제 유지와 커밋

리더와 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여있습니다.  
이렇게 리더와 팔로워를 별도의 그룹으로 나누는 이유는 기본적으로 해당 그룹 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있기 때문입니다.  

ISR 내의 팔로워들은 리더와의 데이터 일치를 유지하기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR 내 모든 팔로워가 메시지를 받을 때까지 기다립니다.  
하지만 팔로워가 네트워크 오류, 브로커 장애 등 여러 이유로 리더로부터 리플리케이션하지 못하는 경우가 발생하면 뒤처진 팔로워는 이미 리더와의 데이터가 불일치한 상태에 놓여 이 팔로워에게 리더를 넘겨준다면 데이터의 정합성이나 메시지 손실 등의 문제가 발생할 수 있습니다.  

따라서 파티션의 리더는 팔로워들이 뒤쳐지지 않고 리플리케이션 동작을 잘하고 있는지를 감시합니다.  
즉 리더에 뒤처지지 않고 잘 따라잡고 있는 팔로워들만이 ISR 그룹에 속하게 되며, 리더에 장애가 발생할 경우 새로운 리더의 자격을 얻을 수 있는 것입니다.  

리더는 팔로워가 리플리케이션 동작을 잘 수행하고 있는지도 판단합니다.  
만약 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는다면, 리더는 해당 팔로워가 리플리케이션 동작에 문제가 발생했다고 판단해 ISR 그룹에서 추방합니다.  

ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게 됩니다.  
마지막 커밋 오프셋의 위치는 하이워터마크(high water mark)라고 부릅니다.  


즉 커밋되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 저장했음을 의미합니다.  
그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있습니다.  

카프카에서 커밋되지 않은 메시지를 컨슈머가 읽을 수 없게 하는 이유는 바로 메시지의 일관성을 유지하기 위해서 입니다.  
그럼 우리는 커밋된 위치를 어떻게 알 수 있을까요?  

모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 `replication-offset-checkpoint`라는 파일에 마지막 커밋 오프셋 위치를 저장합니다.  
```shell
cat /data/kafka-logs/relication-offset-checkpoint
```

만약 특정 토픽 또는 파티션에 복제가 되지 않거나 문제가 있다고 판단되는 경우, replication-offset-checkpoint라는 파일의 내용을 확인하고 리플리케이션되고 있는 다른 브로커들과 비교해 살펴보면, 어떤 브로커, 토픽, 파티션에 문제가 있는지를 파악할 수 있습니다.  

## 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

카프카로 향하는 수많은 메시지의 읽고 쓰기를 처리하는 리더는 매우 빠르게 동작을 합니다.  
이렇게 바쁜 리더가 리플리케이션 동작을 위해 팔로워들과 많은 통신을 주고받거나 리플리케이션 동작에 많은 관여를 한다면, 결과적으로 리더의 성능은 떨어지고 카프카의 장점인 빠른 성능을 내기도 어려울 것입니다.  
따라서 카프카는 리더와 팔로워 간의 리플리케이션 동작을 처리할 때 서로의 통신을 최소화할 수 있도록 설계함으로써 리더의 부하를 줄일 수 있습니다.  

프로듀서가 peter-test01 토픽으로 message1이라는 메시지를 전송했으며, 리더만 메시지를 저장하고 나머지 팔로워들은 아직 리더에게 저장된 메시지를 리플리케이션하기 전입니다.  
팔로워들은 리더에게 0번 오프셋 메시지를 가져오기(fetch) 요청을 보낸 후 새로운 메시지 message1이 있다는 사실을 인지하고 message1 메시지를 리플리케이션 하는 과정입니다.  

현 상태에서 리더는 모든 팔로워가 0번 오프셋 메시지를 리플리케이션하기 위한 요청을 보냈다는 사실을 알고 있습니다.  
하지만 리더는 팔로워들이 0번 오프셋에 대한 리플리케이션 동작을 성공했는지 실패했는지 여부를 알지 못합니다.  

전통적인 메시징 큐 시스템인 래빗MQ의 트랜잭션 모드에서는 모든 미러(mirror, 카프카에서 팔로워에 해당)가 메시지를 받았는지에 대한 ACK를 리턴하므로, 리더는 미러들이 메시지를 받았는지 알 수 있습니다.  
하지만 카프카의 경우에는 리더와 팔로워 사이에서 ACK를 주고받는 통신이 없습니다. 오히려 카프카는 리더와 팔로워 사이에 ACK 통신을 제거함으로써 리플리케이션 동작의 성능을 높였습니다.  

그럼 카프카에서는 어떻게 ACK를 주고받지 않으면서도 안정적으로 리플리케이션 동작을 할 수 있는지 살펴보겠습니다.  

리더는 1번 오프셋의 위치에 두 번째 새로운 메시지인 message2를 프로듀서로부터 받은 뒤 저장합니다.  
0번 오프셋에 대한 리플리케이션 동작을 마친 팔로워들은 리더에게 1번 오프셋에 대한 리플리케이션을 요청합니다.  

팔로워들로부터 1번 오프셋에 대한 리플리케이션 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 리플리케이션 동작이 성공했음을 인지하고, 오프셋 0에 대해 커밋표시를 한 후 하이워터마크를 증가시킵니다.  
팔로워가 0번 오프셋에 대한 리플리케이션을 성공하지 못했다면, 팔로워는 1번 오프셋에 대한 리플리케이션 요청이 아닌 0번 오프셋에 대한 리플리케이션 요청을 보내게 됩니다.  

따라서 리더는 팔로워들이 보내는 리플리케이션 요청의 오프셋을 보고, 팔로워들이 어느 위치의 오프셋까지 리플리케이션을 성공했는지를 인지할 수 있습니다.  
팔로워들로부터 1번 오프셋 메시지에 대한 리플리케이션 요청을 받은 리더는 응답에 0번 오프셋 message1 메시지가 커밋되었다는 내용과 함께 전달합니다.  

리플리케이션의 마지막 과정으로서, 리더의 응답을 받은 모든 팔로워는 0번 오프셋 메시지가 커밋되었다는 사실을 인지하게 되고, 리더와 동일하게 커밋을 표시합니다.  
그리고 1번 오프셋 메시지인 message2를 리플리케이션합니다.  

이렇게 리더와 팔로워 간 메시지의 최신 상태를 유지하게 됩니다.  
이렇게 ACK 통신을 제외한 카프카는 1000개의 메시지에 대한 2000회의 ACK 통신을 줄일 수 있고 메시지를 주고받는 기능에 더욱 집중할 수 있습니다.  

카프카의 또다른 장점은 리플리케이션 동작에서 ACK 통신을 제외했음에도 불구하고 팔로워와 리더간의 리플리케이션 동작이 매우 빠르면서도 신뢰할 수 있다는 점입니다.  
카프카에서 리더와 팔로워들의 리플리케이션 동작 방식은 리더가 푸시하는 방식이 아니라 팔로워가 풀하는 방식으로 동작하는데, 풀 방식을 채택한 이유도 리플리케이션 동작에서 리더의 부하를 줄여주기 위해서입니다.  


## 리더에포크와 복구

리더에포크(LeaderEpoch)는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용됩니다.  
리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자로 표현됩니다.  

해당 리더에포크 정보는 리플리케이션 프로토콜에 의해 전파되고, 새로운 리더가 변경된 후 변경된 리더에 대한 정보는 팔로워에게 전달됩니다.  
리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용됩니다.  
그럼 브로커가 복구 동작을 하는 데 왜 리더에포크가 필요한지 예제를 통해 알아봅시다.  

### 예제 1

peter-test01 토픽의 파티션 수는 1, 리플리케이션 팩터 수는 2, min.insync.replicas는 1입니다.  

먼저 리더 에포크가 없다는 가정하에 장애로부터 복구되는 과정입니다.  
1. 리더는 프로듀서로부터 message1 메시지를 받았고, 0번 오프셋에 저장, 팔로워는 리더에게 0번 오프셋에 대한 가져오기 요청을 합니다.  
2. 가져오기 요청을 통해 팔로워는 message1 메시지를 리더로부터 리플리케이션합니다.  
3. 리더는 하이워터마크를 1로 올립니다.
4. 리더는 프로듀서로부터 다음 메시지인 message2를 받은 뒤 1번 오프셋에 저장합니다.  
5. 팔로워는 다음 메시지인 message2에 대해 리더에게 가져오기 요청을 보내고, 응답으로 리더의 하이워터마크 변화를 감지하고 자신의 하이워터마크도 1로 올립니다.
6. 팔로워는 1번 오프셋의 message2 메시지를 리더로부터 리플리케이션합니다.  
7. 팔로워는 2번 오프셋에 대한 요청을 리더에게 보내고, 요청을 받은 리더는 하이워터마크를 2로 올립니다.  
8. 팔로워는 2번 오프셋인 message2 메시지까지 리플리케이션을 완료했지만, `아직 리더로부터 하이워터마크 2로 올리는 내용은 전달받지 못한 상태`입니다.
9. 예상하지 못한 장애로 팔로워가 다운됩니다.  

복구동작  
1. 팔로워는 자신이 갖고 있는 메시지들 중에서 자신의 워터마크보다 높은 메시지들은 신뢰할 수 없는 메시지로 판단하고 삭제합니다. 따라서 1번 오프셋의 message2는 삭제됩니다.
2. 팔로워는 리더에게 1번 오프셋의 새로운 메시지에 대한 가져오기 요청을 보냅니다.
3. 이 순간 리더였던 브로커가 예상치 못한 장애로 다운되면서, 해당 파티션에 유일하게 남아 있던 팔로워가 새로운 리더로 승격됩니다. 

기존의 리더는 1번 오프셋의 message2를 갖고 있었지만, 팔로워는 message2 없이 새로운 리더로 승격됐습니다.  
결국 뉴리더는 message2를 갖고 있지 않습니다.  
리더와 팔로워간의 리플리케이션이 있음에도 불구하고, 리더가 변경되는 과정을 통해 최종적으로 1번 오프셋의 message2 메시지가 손실된 것입니다.  

리더에포크가 적용된 장애로부터 복구되는 과정  

리더와 팔로워의 리플리케이션 동작 이후, 즉 팔로워가 장애로 종료된 후 막 복구된 상태 이후의 과정입니다.  
앞선 동작에서는 카프카 프로세스가 시작되면서 복구 동작을 통해 하이워터마크보다 높은 메시지를 즉시 삭제했습니다.  

하지만 리더에포크를 사용하는 경우에는 하이워터마크보다 앞에 있는 메시지를 무조건 삭제하는 것이 아니라 리더에게 리더에포크 요청을 보냅니다.  
1. 팔로워는 복구 동작을 하면서 리더에게 리더에포크 요청을 보냅니다.  
2. 요청을 받은 리더는 리더에포크의 응답으로 `1번 오프셋의 message2까지`라고 팔로워에게 보냅니다.  
3. 팔로워는 자신의 하이워터마크보다 높은 1번 오프셋의 message2를 삭제하지 않고 리더의 응답을 확인한 후 message2까지 자신의 하이워터마크를 상향조정합니다.  

리더에포크를 적용하지 않는 경우에는 팔로워가 message2 메시지를 갖고 있음에도 복구 과정에서 하이워터마크보다 높은 메시지를 삭제했습니다.  
하지만 리더에포크를 활용하는 경우에는 삭제동작을 하기에 앞서 리더에포크 요청과 응답을 통해 팔로워의 하이워터마크를 올릴 수 있었고, 메시지 손실은 발생하지 않았습니다.  

### 예제 2

리더만 오프셋 1까지 저장했고, 팔로워는 아직 1번 오프셋 메시지에 대해 리플리케이션 동작을 완료하지 못한 상태입니다.  
현 시점에서 해당 브로커들의 장애가 발생해 리더와 팔로워 모두 다운됐다고 가정하겠습니다.  

브로커가 모두 종료된 후 팔로워가 있던 브로커만 장애에서 복구된 상태입니다. 현재 상태에서 복구동작을 보도록 하겠습니다.  

1. 팔로워였던 브로커가 장애에서 먼저 복구됩니다.  
2. peter-test01 토픽의 0번 파티션에 리더가 없으므로 팔로워는 새로운 리더로 승격됩니다.  
3. 새로운 리더는 프로듀서로부터 다음 메시지 message3을 전달 받고 1번 오프셋에 저장한 후, 자신의 워터마크를 상향조정합니다.  

구 리더였던 브로커의 복구 과정도 보도록 하겠습니다.  

1. 구 리더였던 브로커가 장애에서 복구됩니다.
2. peter-test01 토픽의 0번 파티션에 이미 리더가 있으므로, 복구된 브로커는 팔로워가 됩니다.
3. 리더와 메시지 정합성 확인을 위해 자신의 하이워터마크를 비교해보니 리더의 하이워터마크와 일치하므로, 브로커는 자신이 갖고 있던 메시지를 삭제하지 않습니다.  
4. 리더는 프로듀서로부터 message4 메시지를 받은 후 오프셋2의 위치에 저장합니다.  
5. 팔로워는 오프셋2인 message4를 리플리케이션하기 위해 준비합니다.  

뉴리더는 1번 오프셋 위치에 message3을 갖고 있고, 팔로워는 1번 오프셋 위치에 message2를 갖고 있습니다.  
리더와 팔로워 둘 다 동일한 하이워터마크를 나타내고 있지만 서로의 메시지는 다릅니다.  

`리더와 팔로워가 메시지의 동일한 오프셋 위치만 이용해 복구된다면, 서로의 메시지가 불일치하는 경우가 발생합니다.`
리더에포크를 이용하면 이러한 문제도 해결될까요?  

리더에포크를 이용한 복구 과정  
팔로워가 먼저 복구되어 뉴 리더가 되었고 구 리더였던 브로커가 장애에서 복구된 상태일 때 중요한 점은 뉴리더가 자신이 팔로워일 때의 하이워터마크와 뉴리더일 때의 하이워터마크를 알고 있다는 사실입니다.  
1. 구 리더였던 브로커가 장애에서 복구됩니다.
2. peter-test01 토픽의 0번 파티션에 이미 리더가 있고 자신은 팔로워가 됩니다.
3. 팔로워는 뉴리더에게 리더에포크 요청을 보냅니다.
4. 뉴리더는 0번 오프셋까지 유효하다고 응답합니다.
5. 팔로워는 메시지 일관성을 위해 로컬 파일에서 1번 오프셋인 message2를 삭제합니다. (팔로워는 쓰기 권한이 없으므로, 리더에게 mesaage2를 추가할 수 없습니다.)
6. 팔로워는 리더로부터 1번 오프셋인 message3을 리플리케이션하기 위해 준비합니다.  








